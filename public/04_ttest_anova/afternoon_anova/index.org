#+TITLE: Analysis of Variance
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto
#+OPTIONS: html-preamble:t html-scripts:t html-style:t
#+OPTIONS: html5-fancy:nil tex:t toc:t num:nil
#+HTML_DOCTYPE: xhtml-strict
#+HTML_CONTAINER: div
#+DESCRIPTION:
#+KEYWORDS:
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../../css/my_css.css" />
#+HTML_LINK_HOME: ../../index.html
#+HTML_LINK_UP:   ../../index.html
#+HTML_MATHJAX:
#+HTML_HEAD:
#+HTML_HEAD_EXTRA:
#+SUBTITLE:
#+INFOJS_OPT:
#+CREATOR: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 9.1.5)
#+LATEX_HEADER:
#+PROPERTY: header-args:R :session *R* :exports both :results output

* Tasks                                                            :noexport:
** DONE finish one-way part
   CLOSED: [2018-04-19 Thu 10:07]
** TODO follow up tests?
** TODO model formulae for interactions and main effects
** TODO predictor coding for categorical predictor variables
* Setup                                                            :noexport:

#+begin_src R :exports none :results silent
  library("webex")
  library("tidyverse")
  options(pillar.subtle = TRUE)
  setwd("~/ps_stats/root/04_thursday/morning_ANOVA")

  print.tbl_df <- function(x, ...) {
    print(head(as.data.frame(x), ifelse(nrow(x) > 20, 10, nrow(x)), ...))
    invisible(x)
  }    

  set.seed(500)
#+end_src

#+NAME: hide
#+begin_src R :exports results :results html value :var x = "Solution"
hide(x)
#+end_src

#+NAME: unhide
#+begin_src R :exports results :results html value
unhide()
#+end_src

* Doing ANOVA in R

 There are three ways to perform analysis of variance (ANOVA) in R.

 1. =base::aov()=;
 2. =car::Anova()=; and
 3. =ez::ezANOVA()=.

 (Note =xxx::yyy()= syntax means the function =yyy()= in package =xxx=; =base= is just base R, i.e., the function comes pre-installed with R so to use the function you don't have to load in any packages.)

 If you have balanced data, the factors are uncorrelated (e.g. due to random assignment), and none of your within-subject factors has more that 2 levels, you can use =aov()=.

 If your goal is to replicate the output from SPSS, then your best bet is to use =ez::ezANOVA()=, because it provides Greenhouse-Geiser correction in case you have more than three levels of a within-subject factor, and handles unbalanced data more easily.

 If your goal is to learn a flexible way of analysis that can accommodate all different types of data, forget about ANOVA and learn about mixed-effects modeling.  Just to get this off my chest, here are four reasons why you don't really want to use ANOVA with psychological data:

 1. Doesn't adequately handle more than one random factor at a time,
    and therefore, cannot properly generalize over both subjects and
    stimuli;
 2. Dealing with unbalanced data can be complicated;
 3. Requires that your dependent variable is continuous, and therefore
    is not ideal for discrete scale data or binary data;
 4. Difficult to deal with continuous predictors.

If you have complicated designs and need follow-up tests, there is a newer package, =afex= that might be worth looking into. See [[http://singmann.org/anova-in-r-afex-may-be-the-solution-you-are-looking-for/][this blogpost by =afex= author Henrik Singmann]] on the functionality of =afex=.  To quote Singmann:

#+begin_quote
In the default settings, afex automatically uses appropriate orthogonal contrasts for factors, transforms numerical variables into factors, uses so-called Type III sums of squares, and allows for any number of factors including repeated-measures (or within-subjects) factors and mixed/split-plot designs. Together this guarantees that the ANOVA results correspond to the results obtained from commercial statistical packages such as SPSS or SAS.
#+end_quote

* One-factor ANOVA

** Background

A key way that we attempt to learn from data is to build a *statistical model* that captures relationships among variables.  If you want to learn more about one-factor ANOVA as a kind of a General Linear Model (GLM), check out this free PDF textbook by [[https://www.otago.ac.nz/psychology/otago039309.pdf][Miller and Haden (2006)]] (Chs. 1-3).

** Worked example

Let's assume that you have data from a one-factor design with three-levels. To make this example concrete, let's pretend you are studying how consuming food before an exam affects student performance.  You randomly assign 12 participants to three separate groups (four per group): (1) no food, glass of water only (Control); (2) all-you-can-eat buffet (Buffet); and (3) side salad (Salad). (We chose a small number of participants to simplify computations so that you could repeat the analysis [[*Appendix: One-way ANOVA "by-hand"]["by-hand"]]; obviously if you were going to do this study in real life, you'd need *far more* than 12 participants to make this worthwhile). For your dependent variable, you measure the number of questions answered correctly on an a difficult exam (100 points possible). The exam is administered right after consuming the meal (or drinking water, for the control group).

This is called a *one-factor design* because there is a single factor, which we might call "pre-exam consumption", that has three different levels: water, buffet, and salad. We want to test whether there is any difference in exam performance across the levels of this factor.

Here's how the exam performance looks for each of the three groups:

#+begin_src R :exports results
## this function creates error values for each condition
err_vec <- function(n, sd) {
  etmp <- as.integer(rnorm(n - 1, mean = 0, sd))
  sample(c(etmp, 0L - as.integer(sum(etmp))))
}

.mu <- 50L
.meal_eff <- c(8L, -6L, -2L)
.etmp <- as.integer(rnorm(11, sd = 8))
.err <- c(replicate(3, err_vec(4, 12)))
.Y <- 50 + rep(.meal_eff, each = 4L) + .err
#+end_src

- Control: src_R[:exports results :results value]{paste(.Y[1:4], collapse = ", ")}
- Buffet: src_R[:exports results :results value]{paste(.Y[5:8], collapse = ", ")}
- Salad: src_R[:exports results :results value]{paste(.Y[9:12], collapse = ", ")}

The GLM that we will fit to these data is:

$Y_{ij} = \mu + A_i + e_{ij}$

Where:

- $Y_{ij}$ is the observed value for observation $j$ of group $i$;
- $\mu$ (pronounced "mu") is the population grand mean (estimated by the sample grand mean);
- $A_i$ is the deviation of the population mean of group $i$ from this grand mean;
- $e_{ij}$ is the error or "residual", defined as the observed value ($Y_{ij}$) minus the model prediction ($\hat{Y}_{ij}$).

We will use =ezANOVA()= from the =ez= package to perform a one-way ANOVA. Let's first type our data into a tibble.  Make it look like the table below (Hint: Use the =rep()= to save typing; see =?rep= for examples)

#+begin_src R :exports results
  library("ez")
  library("tidyverse")

  dat <- tibble(Y = .Y,
		A = rep(paste0("A", seq_along(.meal_eff)), each = 4),
		sub_id = 1:length(.Y))
  dat
#+end_src

#+RESULTS:
#+begin_example
 
   Y  A sub_id
1  37 A1      1
2  80 A1      2
3  64 A1      3
4  51 A1      4
5  33 A2      5
6  47 A2      6
7  55 A2      7
8  41 A2      8
9  59 A3      9
10 23 A3     10
11 50 A3     11
12 60 A3     12
#+end_example

#+CALL: hide() :results value

#+begin_src R :exports results
  cat("dat <- tibble(Y = c(", 
      paste(.Y, collapse = ", "), "),\n",
      "              A = rep(c(", paste(paste("\"A", paste(seq_along(.meal_eff), "\"", sep = ""), sep = ""), collapse = ", "), "), each = 4),\n",
      "              sub_id = 1:", length(.Y), ")\n", sep = "")
#+end_src

#+RESULTS:
: dat <- tibble(Y = c(37, 80, 64, 51, 33, 47, 55, 41, 59, 23, 50, 60),
:               A = rep(c("A1", "A2", "A3"), each = 4),
:               sub_id = 1:12)

#+CALL: unhide() :results value

Now look at the help page for =ez::ezANOVA()= and try to figure out how to run the ANOVA. 

 #+BEGIN_SRC R :exports code :eval never
   ?ez::ezANOVA
 #+END_SRC

 #+BEGIN_EXAMPLE
 ezANOVA                   package:ez                   R Documentation

 Compute ANOVA

 Description:

      This function provides easy analysis of data from factorial
      experiments, including purely within-Ss designs (a.k.a. “repeated
      measures”), purely between-Ss designs, and mixed
      within-and-between-Ss designs, yielding ANOVA results, generalized
      effect sizes and assumption checks.

 Usage:

      ezANOVA(
          data
          , dv
          , wid
          , within = NULL
          , within_full = NULL
          , within_covariates = NULL
          , between = NULL
          , between_covariates = NULL
          , observed = NULL
          , diff = NULL
          , reverse_diff = FALSE
          , type = 2
          , white.adjust = FALSE
          , detailed = FALSE
          , return_aov = FALSE
      )
     
 Arguments:

     data: Data frame containing the data to be analyzed.

       dv: Name of the column in ‘data’ that contains the dependent
           variable. Values in this column must be numeric.

      wid: Name of the column in ‘data’ that contains the variable
           specifying the case/Ss identifier. This should be a unique
           value per case/Ss.

   within: Names of columns in ‘data’ that contain predictor variables
           that are manipulated (or observed) within-Ss. If a single
           value, may be specified by name alone; if multiple values,
           must be specified as a .() list.

 within_full: Same as within, but intended to specify the full within-Ss
           design in cases where the data have not already been
           collapsed to means per condition specified by ‘within’ and
           when ‘within’ only specifies a subset of the full design.

 within_covariates: Names of columns in ‘data’ that contain predictor
           variables that are manipulated (or observed) within-Ss and
           are to serve as covariates in the analysis. If a single
           value, may be specified by name alone; if multiple values,
           must be specified as a .() list.

  between: Names of columns in ‘data’ that contain predictor variables
           that are manipulated (or observed) between-Ss. If a single
           value, may be specified by name alone; if multiple values,
           must be specified as a .() list.

 between_covariates: Names of columns in ‘data’ that contain predictor
           variables that are manipulated (or observed) between-Ss and
           are to serve as covariates in the analysis. If a single
           value, may be specified by name alone; if multiple values,
           must be specified as a .() list.

 observed: Names of columns in ‘data’ that are already specified in
           either ‘within’ or ‘between’ that contain predictor variables
           that are observed variables (i.e. not manipulated). If a
           single value, may be specified by name alone; if multiple
           values, must be specified as a .() list. The presence of
           observed variables affects the computation of the generalized
           eta-squared measure of effect size reported by ‘ezANOVA’.

     diff: Names of any variables to collapse to a difference score. If
           a single value, may be specified by name alone; if multiple
           values, must be specified as a .() list. All supplied
           variables must be factors, ideally with only two levels
           (especially if setting the ‘reverse_diff’ argument to TRUE).

 reverse_diff: Logical. If TRUE, triggers reversal of the difference
           collapse requested by ‘diff’. Take care with variables with
           more than 2 levels.

     type: Numeric value (either ‘1’, ‘2’ or ‘3’) specifying the Sums of
           Squares “type” to employ when data are unbalanced (eg. when
           group sizes differ). ‘type = 2’ is the default because this
           will yield identical ANOVA results as ‘type = 1’ when data
           are balanced but ‘type = 2’ will additionally yield various
           assumption tests where appropriate. When data are unbalanced,
           users are warned that they should give special consideration
           to the value of ‘type’. ‘type=3’ will emulate the approach
           taken by popular commercial statistics packages like SAS and
           SPSS, but users are warned that this approach is not without
           criticism.

 white.adjust: Only affects behaviour if the design contains only
           between-Ss predictor variables. If not FALSE, the value is
           passed as the white.adjust argument to ‘Anova’, which
           provides heteroscedasticity correction. See ‘Anova’ for
           details on possible values.

 detailed: Logical. If TRUE, returns extra information (sums of squares
           columns, intercept row, etc.) in the ANOVA table.

 return_aov: Logical. If TRUE, computes and returns an aov object
           corresponding to the requested ANOVA (useful for computing
           post-hoc contrasts).

 Details:

      ANCOVA is implemented by first regressing the DV against each
      covariate (after collapsing the data to the means of that
      covariate's levels per subject) and subtracting from the raw data
      the fitted values from this regression (then adding back the mean
      to maintain scale). These regressions are computed across Ss in
      the case of between-Ss covariates and computed within each Ss in
      the case of within-Ss covariates.

 Value:

      A list containing one or more of the following components:

    ANOVA: A data frame containing the ANOVA results.

 Mauchly's Test for Sphericity: If any within-Ss variables with >2
           levels are present, a data frame containing the results of
           Mauchly's test for Sphericity. Only reported for effects >2
           levels because sphericity necessarily holds for effects with
           only 2 levels.

 Sphericity Corrections: If any within-Ss variables are present, a data
           frame containing the Greenhouse-Geisser and Huynh-Feldt
           epsilon values, and corresponding corrected p-values.

 Levene's Test for Homogeneity: If the design is purely between-Ss, a
           data frame containing the results of Levene's test for
           Homogeneity of variance. Note that Huynh-Feldt corrected
           p-values where the Huynh-Feldt epsilon >1 will use 1 as the
           correction epsilon.

      aov: An aov object corresponding to the requested ANOVA.
      Some column names in the output data frames are abbreviated to
      conserve space:

	DFn         Degrees of Freedom in the numerator (a.k.a. DFeffect).                                                                
	DFd         Degrees of Freedom in the denominator (a.k.a. DFerror).                                                               
	SSn         Sum of Squares in the numerator (a.k.a. SSeffect).                                                                    
	SSd         Sum of Squares in the denominator (a.k.a. SSerror).                                                                   
	F           F-value.                                                                                                              
	p           p-value (probability of the data given the null hypothesis).                                                          
	p<.05       Highlights p-values less than the traditional alpha level of .05.                                                     
	ges         Generalized Eta-Squared measure of effect size (see in references below: Bakeman, 2005).                              
	GGe         Greenhouse-Geisser epsilon.                                                                                           
	p[GGe]      p-value after correction using Greenhouse-Geisser epsilon.                                                            
	p[GGe]<.05  Highlights p-values (after correction using Greenhouse-Geisser epsilon) less than the traditional alpha level of .05. 
	HFe         Huynh-Feldt epsilon.                                                                                                  
	p[HFe]      p-value after correction using Huynh-Feldt epsilon.                                                                   
	p[HFe]<.05  Highlights p-values (after correction using Huynh-Feldt epsilon) less than the traditional alpha level of .05.        
	W           Mauchly's W statistic                                                                                                 
      
 Warning:

      Prior to running (though after obtaining running ANCOVA
      regressions as described in the ‘details’ section), ‘dv’ is
      collapsed to a mean for each cell defined by the combination of
      ‘wid’ and any variables supplied to ‘within’ and/or ‘between’
      and/or ‘diff’. Users are warned that while convenient when used
      properly, this automatic collapsing can lead to inconsistencies if
      the pre-collapsed data are unbalanced (with respect to cells in
      the full design) and only the partial design is supplied to
      ‘ezANOVA’. When this is the case, use ‘within_full’ to specify the
      full design to ensure proper automatic collapsing.

 Author(s):

      Michael A. Lawrence <email: mike.lwrnc@gmail.com>
      Visit the ‘ez’ development site at <
      http://github.com/mike-lawrence/ez>
      for the bug/issue tracker and the link to the mailing list.
 #+END_EXAMPLE

Your results should look like so:

#+NAME: ez1
#+begin_src R :exports results
  ezANOVA(dat, Y, wid = .(sub_id), between = .(A), detailed = TRUE)
#+end_src

#+CALL: hide() :results html value

#+begin_src R :noweb yes :exports code
  <<ez1>>
#+end_src

#+CALL: unhide() :results html value

** Repeat the same analysis using regression                       :noexport:

Previously you learned how to run a regression model using the =lm()= function.  So you could fit a model to these data using the regression formula =Y ~ A=.

#+begin_src R :exports both
  mod <- lm(Y ~ A, dat)
  summary(mod)
#+end_src

Note that =lm()= recognizes that the predictor =A= is not numeric, and so creates the variables =AA2= and =AA3=.  By default, these new variables will be treatment (i.e., dummy) coded.

To test the main effect of A, you need to compare it to a model where the factor A is removed; i.e., a model with just the intercept. You can specify this using the formula =Y ~ 1=. Fit this new model and save it in =mod_noA=.

#+CALL: hide() :results html value

#+begin_src R :exports both
  mod_noA <- lm(Y ~ 1, dat)
#+end_src

#+CALL: unhide() :results html value

Now, the way that you test the main effect of A is to use model comparison, which you do using the =anova()= function. For instance:

#+begin_src R 
  anova(mod, mod_noA)
#+end_src

Compare this result with the result of =ezANOVA()= above.

Let's take this opportunity to redo the analysis, but creating your own predictor variables. For background on creating categorical predictors, [[file:coding_scheme.html][see this overview]].  (How we code the variables should't make any difference for the overall test of the main effect, because there are no interactions.) Create new variables =Ad2v1= and =Ad3v1= in =dat= where the levels of A are coded using deviation coding (baseline category: A1), and store the resulting table in =dat2=.

#+CALL: hide("Hint") :results html value

#+begin_example
mutate(..., Ad1 = if_else(), Ad2 = ...)
#+end_example

#+CALL: unhide() :results html value

#+CALL: hide() :results html value

#+begin_src R :exports both
  dat2 <- dat %>%
    mutate(Ad2v1 = if_else(A == "A2", 2/3, -1/3),
	   Ad3v1 = if_else(A == "A3", 2/3, -1/3))

  dat2
#+end_src

#+RESULTS:
#+begin_example
 
   Y  A sub_id      Ad2v1      Ad3v1
1  37 A1      1 -0.3333333 -0.3333333
2  80 A1      2 -0.3333333 -0.3333333
3  64 A1      3 -0.3333333 -0.3333333
4  51 A1      4 -0.3333333 -0.3333333
5  33 A2      5  0.6666667 -0.3333333
6  47 A2      6  0.6666667 -0.3333333
7  55 A2      7  0.6666667 -0.3333333
8  41 A2      8  0.6666667 -0.3333333
9  59 A3      9 -0.3333333  0.6666667
10 23 A3     10 -0.3333333  0.6666667
11 50 A3     11 -0.3333333  0.6666667
12 60 A3     12 -0.3333333  0.6666667
#+end_example

#+CALL: unhide() :results html value

Next task: re-run the model comparison.

#+CALL: hide() :results html value

#+begin_src R :exports both
  mod2 <- lm(Y ~ Ad2v1 + Ad3v1, dat2)
  anova(mod2, mod_noA)
#+end_src

#+RESULTS:
: Analysis of Variance Table
: 
: Model 1: Y ~ Ad2v1 + Ad3v1
: Model 2: Y ~ 1
:   Res.Df  RSS Df Sum of Sq      F Pr(>F)
: 1      9 2164                           
: 2     11 2580 -2      -416 0.8651 0.4533

#+CALL: unhide() :results html value


* One-way ANOVA "by-hand"

A great way to understand the elements of ANOVA is to do one "by hand". In the old days this would mean doing it with pencil and paper; here we're going to do it by writing code. We'll revisit the data from the [[*Worked example][one-factor example above]].  For convenience, let's repeat the data here:

- Control: src_R[:exports results :results value]{paste(.Y[1:4], collapse = ", ")}
- Buffet: src_R[:exports results :results value]{paste(.Y[5:8], collapse = ", ")}
- Salad: src_R[:exports results :results value]{paste(.Y[9:12], collapse = ", ")}

If you wish to learn more about the concepts of *estimation equations*, *decomposition matrices*, and *sums of squares*, you can find further information in [[https://www.otago.ac.nz/psychology/otago039309.pdf][this free textbook by Miller and Haden]].

We begin by applying estimation equations.  Our estimate of the population grand mean $\mu$, will be based on the grand mean of the sample.  We will call this $\hat{\mu}$. The "hat" that $\mu$ is wearing ($\hat{\mu}$) is just there to remind us that it is not the actual population mean, but an *estimate* of that value derived from our sample.

The estimation equations for our model are:

$\hat{\mu} = Y_{..}$

$\hat{A}_i = Y_{i.} - \hat{\mu}$

$\hat{e}_{ij} = Y_{ij} - (\hat{\mu} + \hat{A}_i) = Y_{ij} - \hat{\mu} - \hat{A}_i$

where

- $Y_{..}$ is the mean of all 12 observations in the sample;
- $Y_{i.}$ is the mean of the 4 observations in group $i$;
- $\hat{A}_i$ is the main effect of $A$ for level $i$, and
- $\hat{e}_{ij}$ is the residual.

Applying these estimation equations to the data above yields the following decomposition matrix:

#+begin_src R :exports results 
.dmx <- tibble(i = rep(1:3, each = 4),
       j = rep(1:4, times = 3),
       Yij = .Y, mu = .mu, Ai = rep(.meal_eff, each = 4),
       err = .err)

.dmx
#+end_src

Note that in this table, =mu= is $\hat{\mu}$, =Ai= is $\hat{A}_i$, and =err= is $\hat{e}_{ij}$.  Take a few moments to understand how this table expresses each of the 12 observed values in our example (the $Y_{ij}$ values) in terms of the linear model $Y_{ij} = \mu + A_i + e_{ij}$.

** Recreate decomposition matrix from the raw data

For this part, your task is to reproduce the tibble shown above, reproduced here:

#+begin_src R :exports results
.dmx
#+end_src

You will do this by typing the observed values into a tibble, and then writing code to add columns with estimates of the individual components (feel free to copy the code you already wrote above). At the end, your table should look exactly like the one above.

You already know how to create a tibble (don't forget to load the tidyverse package first). In case you need to refresh your memory, see [page 2 of this cheatsheet on data input in R](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf).

*** Step 1: Create tibble

Create a tibble named =dmx= (for decomposition matrix). It will eventually contain all of the columns in the one above, but for now, just create the columns =i=, =j=, and =Yij=. You should just type in the values for =Yij=.

 #+CALL: hide("Hint 1") :results html value

 =dmx <- tibble(i = NA, j = NA, Yij = NA)=
 
 #+CALL: unhide() :results html value



 #+CALL: hide("Hint 2") :results html value

 =rep(1:3, each = 4)=
 
 #+CALL: unhide() :results html value


 #+CALL: hide("Solution") :results html value

 #+begin_src R
 library("tidyverse")

 dmx <- tibble(i = rep(1:3, each = 4), 
               j = rep(1:4, times = 3),
               Yij = c(37, 80, 64, 51,
                     33, 47, 55, 41,
                     59, 23, 50, 60)) # TYPE THE VALUES
 #+end_src
 
 #+CALL: unhide() :results html value


*** Step 2: Estimate $\mu$

 Add a column to the table, called =mu= representing $\hat{\mu}$.  Call the resulting table =dmx2=.  Remember that you can add a column to a table using =mutate()=.

 #+CALL: hide("Hint 1") :results html value

 : dmx2 <- dmx %>% 
 :    mutate(mu = ???)
 
 #+CALL: unhide() :results html value



 #+CALL: hide("Solution") :results html value

 #+begin_src R
 dmx2 <- dmx %>%
   mutate(mu = mean(Yij))
 #+end_src
 
 #+CALL: unhide() :results html value



*** Step 3: Enter the estimates $\hat{A}_1$, $\hat{A}_2$, $\hat{A}_3$

 Add a column to the table =dmx2= called =Ai=, with the three estimates for $\hat{A}_i$.  Store the resulting tibble in =dmx3=.

 #+CALL: hide("Hint") :results html value

 - don't forget to =ungroup()= at the end of the pipeline
 
 #+CALL: unhide() :results html value



 #+CALL: hide("Solution") :results html value

 #+begin_src R
 dmx3 <- dmx2 %>%
   group_by(i) %>% # calculate different values for each i
   mutate(Ai = mean(Yij) - mu) %>%
   ungroup()
 #+end_src
 
 #+CALL: unhide() :results html value



*** Step 4: Calculate \(\hat{e}_{ij}\)s

Now add a column =err= to =dmx3= and store the result in =dmx4=. The column =err= should contain the residuals (the difference between the observed and fitted values)

 #+CALL: hide("Hint") :results html value

 $\hat{e}_{ij} = Y_{ij} - \hat{Y}_{ij}$

 $\hat{Y}_{ij} = \hat{\mu} + \hat{A}_i$ 
 
 #+CALL: unhide() :results html value



 #+CALL: hide("Solution") :results html value

 #+begin_src R
 dmx4 <- dmx3 %>%
   mutate(err = Yij - (mu + Ai))

 dmx4
 #+end_src
 
 #+CALL: unhide() :results html value



 Display =dmx4= in the console and compare it to the table above.  How did you do?

*** Step 5: Sums of squares

Sums of squares are used in calculations for performing tests on model components.

Square the values in the columns =Yij=, =mu=, =Ai=, and =err= in =dmx4=, then sum up the squared values for each of these columns.  Here is an example of how you would square a value =x= in R: =x^2=.  The =^2= means take x to the power of 2.  So typing =3^2= in the console will give you =9= (try it if you're unsure).

 #+CALL: hide("Hint 1") :results html value

 : dmx4 %>% mutate(Yij2 = Yij^2, ...)
 
 #+CALL: unhide() :results html value



 #+CALL: hide("Hint 2") :results html value

 : summarise(ss_Y = sum(Yij2), ss_mu = ...)
 
 #+CALL: unhide() :results html value



 #+CALL: hide("Solution") :results html value

 #+begin_src R
 sstbl <- dmx4 %>%
   mutate(Yij2 = Yij^2,
          mu2 = mu^2,
          Ai2 = Ai^2,
          err2 = err^2) %>%
   select(Yij2, mu2, Ai2, err2) %>%
   summarise(ss_Y = sum(Yij2),
             ss_mu = sum(mu2),
             ss_Ai = sum(Ai2),
             ss_err = sum(err2))

 ## here is a super cool way to do the same thing,
 ## using dplyr's "scoping" technique 
 ## (see ?dplyr::scoped and ?dplyr::summarise_all)
 sstbl <- dmx4 %>%
   select(Yij:err) %>%
   summarise_all(funs(sum(.^2)))

 sstbl
 #+end_src
 
 #+CALL: unhide() :results html value

*** Final step: Calculate \(F\) statistics

\(F = \frac{MS_{A}}{MS_{err}}\)

\(MS_A = \frac{MS_A}{df_A}\)

\(MS_{err} = \frac{MS_{err}}{df_{err}}\)

Get the p-value using the appropriate probability distribution for the \(F\) distribution (Hint: remember =dnorm()=, =pnorm()=, and =qnorm()=?)

Your results should *exactly* reproduce the ones we got from =ezANOVA()=.

#+CALL: hide("Hint (F function)") :results html value

=?pf=

#+CALL: unhide() :results html value

#+CALL: hide("Solution") :results html value

#+begin_src R
  ms_a <- (sstbl %>% pull(Ai)) / 2L # 2 dfs
  ms_err <- (sstbl %>% pull(err)) / 9L # 9 dfs

  f_ratio <- ms_a / ms_err
  p_value <- pf(f_ratio, 2, 9, lower.tail = FALSE)

  cat("F(2, 9) =", round(f_ratio, 2), ", p =", round(p_value, 3), "\n")
#+end_src

#+RESULTS:
: F(2, 9) = 0.87 , p = 0.453

#+CALL: unhide() :results html value


* 2x2 ANOVA                                                        :noexport:

Consider the following hypothetical data from a 2x2 design (both factors between subjects) from an imaginary study investigating the effects of a new (vs. old) educational programme for rural (vs. urban) schoolchildren.

Copy and run the code below to create the table `edat`.

#+begin_src R
  scores = as.integer(round(rnorm(16, 75, 12)))

  edat <- tibble(
    student_id = 1:16,
    programme = rep(c("old", "new"), each = 8),
    setting = rep(rep(c("rural", "urban"), each = 4), 2),
    test_score = case_when(scores < 0 ~ 0L,
		      scores > 100 ~ 100L,
		      TRUE ~ scores))               
#+end_src

Now analyze the data using =ezANOVA()=.  See =?ezANOVA= for help.

#+CALL: hide("Hint about specifying the IVs") :results html value

#+begin_example
  between = .(programme, setting)
#+end_example

#+CALL: unhide() :results html value

#+CALL: hide() :results html value

#+begin_src R
  emod <- ezANOVA(edat, test_score, student_id,
		  between = .(programme, setting))

  emod
#+end_src

#+CALL: unhide() :results html value

OK let's repeat the analysis using regression and =lm()= instead of =ezANOVA=, but this time, we'll code up our own categorical predictors: =P= for =programme= and =S= for setting.  Use deviation coding, and store the transformed table in =edat2=.

#+CALL: hide() :results html value

#+begin_src R 
  edat2 <- edat %>%
    mutate(P = if_else(programme == "new", 1/2, -1/2),
	   S = if_else(setting == "rural", 1/2, -1/2))
#+end_src

#+CALL: unhide() :results html value

Now run the regression.

#+CALL: hide() :results html value

#+begin_src R 
  edat2_mod <- lm(test_score ~ P * S, edat2)
  summary(edat2_mod)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = test_score ~ P * S, data = edat2)

Residuals:
    Min      1Q  Median      3Q     Max 
-13.500  -9.000  -1.875   7.812  17.500 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.350e+01  2.980e+00  24.661 1.19e-11 ***
P           -6.250e+00  5.961e+00  -1.049    0.315    
S            4.462e-15  5.961e+00   0.000    1.000    
P:S          5.000e-01  1.192e+01   0.042    0.967    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 11.92 on 12 degrees of freedom
Multiple R-squared:  0.08405,	Adjusted R-squared:  -0.1449 
F-statistic: 0.367 on 3 and 12 DF,  p-value: 0.7781
#+end_example

#+CALL: unhide() :results html value

Compare your results to =ezANOVA= above.

* Mixed-design ANOVA using =ez::ezANOVA()=                         :noexport:

Let's look at a more complicated 'mixed' experiment design.

** The ANT dataset

 We will be using /simulated/ data from the Attention Network Test (ANT) by Fan et al. (2002), which is included as part of the =ez= package.

 #+BEGIN_SRC R :eval never :exports code
   ?ez::ANT
 #+END_SRC

 #+BEGIN_EXAMPLE
 ANT                     package:ez                     R Documentation

 ANT data

 Description:

      _Simulated_ data from then Attention Network Test (see reference
      below), consisting of 2 within-Ss variables (“cue” and “flank”), 1
      between-Ss variable (“group”) and 2 dependent variables (response
      time, “rt”, and whether an error was made, “error”)

 Usage:

      data(ANT)
     
 Format:

      A data frame with 5760 observations on the following 10 variables.

      ‘subnum’ a factor with levels ‘1’ ‘2’ ‘3’ ‘4’ ‘5’ ‘6’ ‘7’ ‘8’ ‘9’
           ‘10’ ‘11’ ‘12’ ‘13’ ‘14’ ‘15’ ‘16’ ‘17’ ‘18’ ‘19’ ‘20’

      ‘group’ a factor with levels ‘Control’ ‘Treatment’

      ‘block’ a numeric vector

      ‘trial’ a numeric vector

      ‘cue’ a factor with levels ‘None’ ‘Center’ ‘Double’ ‘Spatial’

      ‘flank’ a factor with levels ‘Neutral’ ‘Congruent’ ‘Incongruent’

      ‘location’ a factor with levels ‘down’ ‘up’

      ‘direction’ a factor with levels ‘left’ ‘right’

      ‘rt’ a numeric vector

      ‘error’ a numeric vector

 Author(s):

      Michael A. Lawrence <email: mike.lwrnc@gmail.com>
      Visit the ‘ez’ development site at <
      http://github.com/mike-lawrence/ez>
      for the bug/issue tracker and the link to the mailing list.

 References:

      J Fan, BD McCandliss, T Sommer, A Raz, MI Posner (2002). Testing
      the efficiency and independence of attentional networks. _Journal
      of Cognitive Neuroscience_, *14*, 340-347.

 Examples:

      data(ANT)
      head(ANT)
      ezPrecis(ANT)
 #+END_EXAMPLE

** Explore the data

 Let's set up the session environment. The dataset is stored somewhere in the package, and we need to make it accessible to our session using the =data()= function.  

 #+BEGIN_SRC R
   library("ez") # for inferential analysis using ANOVA
   library("tidyverse")

   data(ANT)
 #+END_SRC

 =ez::ezPrecis()= and =dplyr::glimpse()= give useful information about the dataset.

 #+begin_src R
   ezPrecis(ANT)
 #+end_src

 #+begin_src R
   glimpse(ANT)
 #+end_src


 Now let's keep only the accurate trials:

 #+begin_src R
   ## keep only the accurate trials
   ANT_acc <- filter(ANT, error == 0)
 #+end_src

 How many subjects do we have?

 #+BEGIN_SRC R
   ANT_acc %>% count(subnum)
 #+END_SRC

 #+RESULTS:
 #+begin_example
 Source: local data frame [20 x 2]

    subnum     n
    (fctr) (int)
 1       1   264
 2       2   255
 3       3   261
 4       4   258
 5       5   251
 6       6   259
 7       7   259
 8       8   265
 9       9   261
 10     10   264
 11     11   266
 12     12   260
 13     13   256
 14     14   237
 15     15   263
 16     16   264
 17     17   260
 18     18   259
 19     19   264
 20     20   265
 #+end_example

 Let's see which factors are within and which are between.

 #+BEGIN_SRC R
   ANT_acc %>% count(subnum, group, cue, flank) %>%
      filter(subnum %in% c(1, 11)) %>%
      print(n = 100)
 #+END_SRC

 #+RESULTS:
 #+begin_example
  Source: local data frame [24 x 5]
 Groups: subnum, group, cue [8]

    subnum     group     cue       flank     n
    (fctr)    (fctr)  (fctr)      (fctr) (int)
 1       1 Treatment    None     Neutral    18
 2       1 Treatment    None   Congruent    21
 3       1 Treatment    None Incongruent    23
 4       1 Treatment  Center     Neutral    22
 5       1 Treatment  Center   Congruent    23
 6       1 Treatment  Center Incongruent    23
 7       1 Treatment  Double     Neutral    24
 8       1 Treatment  Double   Congruent    21
 9       1 Treatment  Double Incongruent    22
 10      1 Treatment Spatial     Neutral    22
 11      1 Treatment Spatial   Congruent    21
 12      1 Treatment Spatial Incongruent    24
 13     11   Control    None     Neutral    19
 14     11   Control    None   Congruent    23
 15     11   Control    None Incongruent    23
 16     11   Control  Center     Neutral    23
 17     11   Control  Center   Congruent    24
 18     11   Control  Center Incongruent    22
 19     11   Control  Double     Neutral    24
 20     11   Control  Double   Congruent    23
 21     11   Control  Double Incongruent    19
 22     11   Control Spatial     Neutral    21
 23     11   Control Spatial   Congruent    22
 24     11   Control Spatial Incongruent    23
 #+end_example

 So, we have a mixed design with one between factor (=group=) and two within factors (=cue=, =flank=).  Both of our within-subject factors have more than two levels, so we're going to want to correct for possible violations of sphericity (Greenhouse-Geiser).  Just to make things more fun, the data are unbalanced now that we've thrown out trials with inaccurate responses.  So if we can handle these data, we can handle anything...

 Before doing any analysis, let's look at the cell means.

 #+BEGIN_SRC R
   ANT_means <- ANT_acc %>%
       group_by(group, cue, flank) %>%
       summarise(mRT = mean(rt)) %>% ungroup()
 #+END_SRC

 #+BEGIN_SRC R :exports results
   ANT_means %>% as.data.frame()
 #+END_SRC

 #+RESULTS:
 #+begin_example
	group     cue       flank      mRT
 1    Control    None     Neutral 427.9266
 2    Control    None   Congruent 429.4866
 3    Control    None Incongruent 498.9557
 4    Control  Center     Neutral 378.6429
 5    Control  Center   Congruent 380.7638
 6    Control  Center Incongruent 483.3961
 7    Control  Double     Neutral 379.5458
 8    Control  Double   Congruent 372.6635
 9    Control  Double Incongruent 473.1597
 10   Control Spatial     Neutral 340.7174
 11   Control Spatial   Congruent 343.4821
 12   Control Spatial Incongruent 411.3912
 13 Treatment    None     Neutral 428.1812
 14 Treatment    None   Congruent 426.1165
 15 Treatment    None Incongruent 495.8052
 16 Treatment  Center     Neutral 382.8072
 17 Treatment  Center   Congruent 376.5433
 18 Treatment  Center Incongruent 452.4431
 19 Treatment  Double     Neutral 369.8689
 20 Treatment  Double   Congruent 378.3121
 21 Treatment  Double Incongruent 446.6856
 22 Treatment Spatial     Neutral 339.3939
 23 Treatment Spatial   Congruent 337.8150
 24 Treatment Spatial Incongruent 414.6278
 #+end_example

 Plot them...

 #+BEGIN_SRC R :results output graphics file :file ant_means.png :width 600 :height 300
   ggplot(ANT_means, aes(flank, mRT, colour = cue)) +
     geom_line(aes(group = cue)) +
     geom_point(aes(shape = cue), size = 3) +
     facet_wrap(~group)
 #+END_SRC

** Analysis using =ezANOVA()=

 So what might we expect from this plot?  Main effects of flank and cue, maybe even a three-way interaction?

 Note that we should be using Type III ss if we want to replicate SPSS output.

 (Note that this is using RM-ANOVA and not mixed-model ANOVA; in the latter case you would have different error terms for the different effects.)

 #+BEGIN_SRC R
   rt_anova <- ezANOVA(ANT_acc, rt, subnum, within = .(cue, flank),
                       between = group, type = 3)

   print(rt_anova)
 #+END_SRC

 #+RESULTS:
 #+begin_example
  Warning: Collapsing data to cell means. *IF* the requested effects are a subset of the full design, you must use the "within_full" argument, else results may be inaccurate.
 $ANOVA
            Effect DFn DFd           F            p p<.05        ges
 2           group   1  18   18.430592 4.377562e-04     * 0.07633358
 3             cue   3  54  516.605213 1.005518e-39     * 0.89662286
 5           flank   2  36 1350.598810 1.386546e-34     * 0.92710583
 4       group:cue   3  54    2.553236 6.497492e-02       0.04110445
 6     group:flank   2  36    8.768499 7.900829e-04     * 0.07627434
 7       cue:flank   6 108    5.193357 9.938494e-05     * 0.11436699
 8 group:cue:flank   6 108    6.377225 9.012515e-06     * 0.13686958

 $`Mauchly's Test for Sphericity`
            Effect         W         p p<.05
 3             cue 0.7828347 0.5366835      
 4       group:cue 0.7828347 0.5366835      
 5           flank 0.8812738 0.3415406      
 6     group:flank 0.8812738 0.3415406      
 7       cue:flank 0.1737053 0.1254796      
 8 group:cue:flank 0.1737053 0.1254796      

 $`Sphericity Corrections`
            Effect       GGe        p[GG] p[GG]<.05       HFe        p[HF]
 3             cue 0.8652559 1.115029e-34         * 1.0239520 1.005518e-39
 4       group:cue 0.8652559 7.472046e-02           1.0239520 6.497492e-02
 5           flank 0.8938738 3.763312e-31         * 0.9858964 3.964046e-34
 6     group:flank 0.8938738 1.297752e-03         * 0.9858964 8.438369e-04
 7       cue:flank 0.6022111 1.546166e-03         * 0.7721473 4.745714e-04
 8 group:cue:flank 0.6022111 3.424499e-04         * 0.7721473 7.170939e-05
   p[HF]<.05
 3         *
 4          
 5         *
 6         *
 7         *
 8         *
 #+end_example

** Compare to SPSS output

 First we have to reshape the data from long to wide.  Better to do this in R using =tidyr::spread()= than to try to do this in SPSS!

 #+BEGIN_SRC R
   ANT_agg <- ANT_acc %>%
     group_by(subnum, group, cue, flank) %>%
     summarise(RT = mean(rt)) %>% ungroup()

   for_spss <- ANT_agg %>%
     mutate(cond = paste(cue, flank, sep = "_")) %>%
     select(subnum, group, cond, RT) %>%
     spread(cond, RT)

   write.csv(for_spss, "for_spss.csv", row.names = FALSE)
   glimpse(for_spss)
 #+END_SRC

 #+RESULTS:
 #+begin_example
 Observations: 20
 Variables: 14
 $ subnum              (fctr) 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,...
 $ group               (fctr) Treatment, Treatment, Treatment, Treatment, Tr...
 $ Center_Congruent    (dbl) 368.6432, 389.7828, 367.9923, 383.2157, 394.601...
 $ Center_Incongruent  (dbl) 473.3917, 455.6889, 436.5402, 438.4868, 471.384...
 $ Center_Neutral      (dbl) 385.2220, 374.2402, 371.2286, 384.8149, 378.096...
 $ Double_Congruent    (dbl) 378.9103, 384.1513, 356.3303, 399.3815, 370.688...
 $ Double_Incongruent  (dbl) 455.3791, 432.9680, 432.6974, 445.8616, 447.052...
 $ Double_Neutral      (dbl) 378.9357, 356.9811, 392.3132, 370.2583, 370.638...
 $ None_Congruent      (dbl) 423.8810, 441.9487, 428.1394, 433.2586, 436.916...
 $ None_Incongruent    (dbl) 490.8676, 483.2877, 503.2587, 498.5326, 499.016...
 $ None_Neutral        (dbl) 433.4563, 436.4319, 428.6568, 415.5254, 429.138...
 $ Spatial_Congruent   (dbl) 350.1803, 337.6057, 334.5602, 342.8093, 329.854...
 $ Spatial_Incongruent (dbl) 407.1928, 421.0479, 431.4388, 407.0074, 418.313...
 $ Spatial_Neutral     (dbl) 328.0655, 339.3997, 331.3369, 320.7324, 332.479...
 #+end_example

 Here is how the data look:

 [[file:01_spss_data.png]]

 And then we choose =Repeated Measures= from the analysis menu:

 [[file:02_spss_menu.png]]

 Define our within IVs:

 [[file:03_spss_repeated_measures.png]]

 Results:

 [[file:04_spss_mauchly.png]]

 [[file:05_spss_within.png]]

 [[file:06_spss_between.png]]

** Mixed-effects analysis

 #+BEGIN_SRC R
   library("afex")

   ## see ?afex::mixed
   ## NB: random-intercept-only model justified here because of
   ##     pre-aggregation of the data
   mod <- mixed(RT ~ group * cue * flank + (1 | subnum), ANT_agg, type = 3,
		method = "KR")

   print(mod)
 #+END_SRC

 #+RESULTS:
 #+begin_example
 Fitting 8 (g)lmer() models:
 [........]
 Obtaining 7 p-values:
 [.......]
            Effect      F ndf    ddf F.scaling p.value
 1           group   0.01   1 216.00      1.00     .94
 2             cue 107.14   3 198.00      1.00  <.0001
 3           flank 135.42   2 198.00      1.00  <.0001
 4       group:cue   1.37   3 198.00      1.00     .25
 5     group:flank   0.09   2 198.00      1.00     .92
 6       cue:flank   9.40   6 198.00      1.00  <.0001
 7 group:cue:flank   5.71   6 198.00      1.00  <.0001
 #+end_example


* 

#+begin_export html
 <script>

 /* update total correct if #total_correct exists */
 update_total_correct = function() {
   if (t = document.getElementById("total_correct")) {
     t.innerHTML =
       document.getElementsByClassName("correct").length + " of " +
       document.getElementsByClassName("solveme").length + " correct";
   }
 }

 /* solution button toggling function */
 b_func = function() {
   var cl = this.parentElement.classList;
   if (cl.contains('open')) {
     cl.remove("open");
   } else {
     cl.add("open");
   }
 }

 /* function for checking solveme answers */
 solveme_func = function(e) {
   var real_answers = JSON.parse(this.dataset.answer);
   var my_answer = this.value;
   var cl = this.classList;
   if (cl.contains("ignorecase")) {
     my_answer = my_answer.toLowerCase();
   }
   if (cl.contains("nospaces")) {
     my_answer = my_answer.replace(/ /g, "");
   }
  
   if (my_answer !== "" & real_answers.includes(my_answer)) {
     cl.add("correct");
   } else {
     cl.remove("correct");
   }
   update_total_correct();
 }

 window.onload = function() {
   /* set up solution buttons */
   var buttons = document.getElementsByTagName("button");

   for (var i = 0; i < buttons.length; i++) {
     if (buttons[i].parentElement.classList.contains('solution')) {
       buttons[i].onclick = b_func;
     }
   }
  
   /* set up solveme inputs */
   var solveme = document.getElementsByClassName("solveme");

   for (var i = 0; i < solveme.length; i++) {
     /* make sure input boxes don't auto-anything */
     solveme[i].setAttribute("autocomplete","off");
     solveme[i].setAttribute("autocorrect", "off");
     solveme[i].setAttribute("autocapitalize", "off"); 
     solveme[i].setAttribute("spellcheck", "false");
     solveme[i].value = "";
    
     /* adjust answer for ignorecase or nospaces */
     var cl = solveme[i].classList;
     var real_answer = solveme[i].dataset.answer;
     if (cl.contains("ignorecase")) {
       real_answer = real_answer.toLowerCase();
     }
     if (cl.contains("nospaces")) {
       real_answer = real_answer.replace(/ /g, "");
     }
     solveme[i].dataset.answer = real_answer;
    
     /* attach checking function */
     solveme[i].onkeyup = solveme_func;
     solveme[i].onchange = solveme_func;
   }
  
   update_total_correct();
 }

 </script>
#+end_export

