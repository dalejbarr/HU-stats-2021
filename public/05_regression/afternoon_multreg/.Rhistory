library("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
.y <- function() "<span style=\"color:green\">yes</span>"
.n <- function() "<span style=\"color:red\">**NO**</span>"
.cc <- function(var, class) {
.var <- deparse(substitute(var))
result <- .n()
if (exists(.var)) {
if (inherits(var, class)) {
result <- .y()
}
}
result
}
.csn <- function(x) {
.x <- deparse(substitute(x))
result <- .n()
if (exists(.x)) {
if (is.vector(x)) {
if (is.numeric(x) && (length(x) == 1L)) {
result <- .y()
}
}
}
result
}
.ce <- function(x) {
ex <- sapply(x, exists)
if (any(!ex)) {
allnames <- paste0("`", names(ex)[!ex], "`")
str <- paste0(paste(allnames, collapse = ","), "not found; check spelling and capitalization")
paste0("<span style=\"color:red\">", str, "</span>")
} else .y()
}
.cnmx <- function(x) { # numeric matrix
.x <- deparse(substitute(x))
if (exists(.x)) {
if (is.numeric(x) && is.matrix(x)) .y() else .n()
}
}
pers_traits <- read_csv("id,dimension,score\n1,O,2.8\n1,CONS,3\n1,extrav,3.5\n1,Agree,4\n1,Neuro,5.4\n2,O,5.9\n2,CONS,2.6\n2,extrav,3.4\n2,Agree,5\n2,Neuro,3.2\n3,O,3.1\n3,CONS,1.7\n3,extrav,4.7\n3,Agree,3.4\n3,Neuro,4.8\n4,O,3.7\n4,CONS,5.4\n4,extrav,4.7\n4,Agree,4.5\n4,Neuro,3.8\n5,O,3.1\n5,CONS,5\n5,extrav,5.1\n5,Agree,4.2\n5,Neuro,4.7\n6,O,2.6\n6,CONS,4.4\n6,extrav,3.7\n6,Agree,5.5\n6,Neuro,5.8\n7,O,2.2\n7,CONS,3.3\n7,extrav,3.3\n7,Agree,3.7\n7,Neuro,2.8\n8,O,2.9\n8,CONS,4.7\n8,extrav,5.3\n8,Agree,3.4\n8,Neuro,4.9\n9,O,4.6\n9,CONS,3.1\n9,extrav,2.5\n9,Agree,3.7\n9,Neuro,3.2\n10,O,5\n10,CONS,5.3\n10,extrav,4.8\n10,Agree,4.6\n10,Neuro,5", col_types = "icd")
sm_att <- read_csv("id,sma_score\n1,17\n2,13\n3,14\n4,14\n5,14\n6,14\n7,15\n8,17\n9,17\n10,19", col_types = "ii")
.allvars <- c("cormatrix", "corr_WX", "covar_YZ", "stdev_Z", "b_0", "b_1", "covar", "simdat")
rm(list = ls())
library(corrr)
both_wide <- pers_traits %>%
pivot_wider(names_from = dimension, values_from = score) %>%
inner_join(sm_att, "id")
both_wide %>%
select(-id) %>%
correlate() %>%
shave() %>%
fashion()
pairs(~ O + CONS + extrav +Agree + Neuro, both_wide)
pairs(~ O + CONS + extrav +Agree + Neuro + sma_score, both_wide)
cormatrix <- both_wide %>%
select(-id) %>%
correlate() %>%
shave() %>%
fashion()
corr_WX <- (7.2/12)
covar_YZ <- -0.6
stdev_Z <- 1
b1 <- -0.7 * (77.6 / 83.032)
b0 <- 79.4 - b1 * 52.4
b_0 <- 79.4 - b1 * 52.4
b_1 <- -0.7 * (77.6 / 83.032)
correlation * sdX * sdY, sdY^2), nrow = 2)
covar <- matrix(c(9^2, 0.4 * 9 * 7,
0.4 * 9 * 7, 7^2), nrow = 2)
sigma <- matrix(c(81, 25.2, 25.2, 49), nrow = 2)
simulated_data <- MASS::mvrnorm(9, c(X = 64, Y = 158), sigma)
simulated_data
simdat <- MASS::mvrnorm(9, c(X = 64, Y = 158), sigma)
correlation * sdX * sdY, sdY^2), nrow = 2)
covar <- matrix(c(9^2, 0.4 * 9 * 7,
0.4 * 9 * 7, 7^2), nrow = 2)
sigma <- matrix(c(81, 25.2, 25.2, 49), nrow = 2)
simulated_data <- MASS::mvrnorm(9, c(X = 64, Y = 158), sigma)
simulated_data
simdat <- MASS::mvrnorm(9, c(X = 64, Y = 158), sigma)
covar <- matrix(c(9^2, 0.4 * 9 * 7,
0.4 * 9 * 7, 7^2), nrow = 2)
sigma <- matrix(c(81, 25.2, 25.2, 49), nrow = 2)
simulated_data <- MASS::mvrnorm(9, c(X = 64, Y = 158), sigma)
simdat <- MASS::mvrnorm(9, c(X = 64, Y = 158), sigma)
devtools::install_github("dalejbarr/truthiness")
R
t.test(rnorm(5), rnorm(5))
q()
n
library(truthiness)
help(package = "truthiness")
help(package = "truthiness")
install.packages("spotifyr")
devtools::install_github("charlie86/spotifyr")
help(package = "spotifyr")
?get_user_playlists
?get_user_playlists
get_user_playlists("dalebarr", authorization = access_token)
help(package = "spotifyr")
1
q()
n
help(package = "truthiness")
help(package = "truthiness")
library(truthiness)
?preprocess
library(truthiness)
?preprocess
help(package = "truthiness")
help(package = "truthiness")
devtools::install_github("dalejbarr/truthiness", upgrade = "never")
sess <- sessions %>%
filter(keep) %>%  ## apply exclusions
select(-(keep:chk_notmanex)) ## remove columns we don't need
knitr::opts_chunk$set(echo = TRUE)
if (!requireNamespace("truthiness")) {
stop("You must install the 'truthiness' package to run this analysis. \n",
"devtools::install_github('dalejbarr/truthiness')")
}
if (!requireNamespace("lme4")) {
stop("You must install the 'lme4' package to run this analysis.")
}
if (!requireNamespace("tidyverse")) {
stop("You must install the 'tidyverse' package to run this analysis.")
}
if (!requireNamespace("corrr")) {
stop("You must install the 'corrr' package to run this analysis.")
}
library("lme4")
library("truthiness")
library("tidyverse")
library("corrr")
sess <- sessions %>%
filter(keep) %>%  ## apply exclusions
select(-(keep:chk_notmanex)) ## remove columns we don't need
phase <- phases %>%
filter(keep) %>%  ## apply exclusions
select(-(keep:reason_for_manual_exclusion)) ## remove columns we don't need
phase <- phases %>%
filter(keep) %>%  ## apply exclusions
select(-(keep:chk_notmanex)) ## remove columns we don't need
stimulus_conditions
packageVersion("rmarkdown")
packageVersion("knitr")
packageVersion("shiny")
install.packages("truthiness")
library(truthiness)
?truth_trajectory_models
summary(truth_trajectory_models$ix_base)
library(ordinal)
summary(truth_trajectory_models$ix_base)
q()
n
q()
n
63/365
377 / (36/365)
63 / 365
68/365
377 / (68 / 365)
q()
n
?brief_entries
tribble(~ activity, ~ year, ~ description) %>%
brief_entries("Associated Editor", "2014-2019", "Behavior Research Methods")
# Chunk 1: setup
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(vitae)
# Chunk 2
library(tibble)
library(dplyr)
library(purrr)
tribble(
~ Degree, ~ Year, ~ Institution, ~ Where,
"B.A., Psychology", "1989-1993", "University of California, Santa Cruz", "Santa Cruz, CA, USA",
"Ph.D., Psychology (summa cum laude)", "1994-1999", "The University of Chicago", "Chicago, IL, USA",
"Postdoctoral Fellow", "1999-2002", "Beckman Institute, University of Illinois at Urbana-Champaign", "Urbana-Champaign, IL, USA",
"Assistant Professor", "2002-2009", "University of California, Riverside", "Riverside, CA, USA",
"Associate Professor", "2009-2010", "University of California, Riverside", "Riverside, CA, USA",
"Senior Lecturer", "2010-present", "University of Glasgow", "Glasgow, UK") %>%
detailed_entries(Degree, Year, Where, Institution)
tribble(~ activity, ~ year, ~ description) %>%
brief_entries("Associated Editor", "2014-2019", "Behavior Research Methods")
tribble(~ activity, ~ year, ~ description,
"Associate Editor", "2014-2019", "Behavior Research Methods") %>%
brief_entries(activity, year, description)
tribble(~who, ~breakfast, ~lunch, ~dinner, ~snacks,
"luke",	403,	556,	780,	447,
"leia",	0,	580,	803,	201,
"yoda",	946,	311,	1442,	0)
library("tidyverse")
tribble(~who, ~breakfast, ~lunch, ~dinner, ~snacks,
"luke",	403,	556,	780,	447,
"leia",	0,	580,	803,	201,
"yoda",	946,	311,	1442,	0)
calories <- tribble(~who, ~breakfast, ~lunch, ~dinner, ~snacks,
"luke",	403,	556,	780,	447,
"leia",	0,	580,	803,	201,
"yoda",	946,	311,	1442,	0)
calories
knitr::kable(calories)
food_diary <- tribble(~who, ~breakfast, ~lunch, ~dinner, ~snacks,
"luke",	403,	556,	780,	447,
"leia",	0,	580,	803,	201,
"yoda",	946,	311,	1442,	0)
knitr::kable(food_diary)
food_diary %>%
pivot_longer(breakfast:snacks,
names_to = "meal",
values_to = "calories")
food_diary %>%
pivot_longer(breakfast:snacks,
names_to = meal,
values_to = calories)
"
food_diary %>%
pivot_longer(breakfast:snacks,
names_to = "meal",
values_to = "calories")
food_diary %>%
pivot_longer(breakfast:snacks,
names_to = "meal",
values_to = "calories")
subjects <- tribble(~id, ~age, ~nationality,
1L, 21L, "canadian",
2L, 32L, "german",
3L, 19L, "united kingdom")
trials <- tibble(id = rep(c(1L, 3L), each = 3),
stimulus = rep(paste0("word", 1:3), 2))
trials
trials <- tibble(id = rep(c(1L, 3L), each = 3),
stimulus = rep(paste0("word", 1:3), 2),
rt = sample(800:1200, 6))
trials
result <- inner_join(subjects, trials, "id")
result
wrong1 <- left_join(subjects, trials, "id")
wrong1
result
result %>%
select(id, age, nationality)
crossing(subjects, trials)
crossing(subjects, trials %>% select(-id))
vignette(package = "ggplot2")
?dbinom
dbinom(5, 7, prob = 1/2)
dbinom(5, 7, prob = 1/2)
dbinom(6, 7, prob = 1/2)
dbinom(7, 7, prob = 1/2)
dbinom(5:7, 7, prob = 1/2)
sum(dbinom(5:7, 7, prob = 1/2))
sum(dbinom(80:100, 100, prob = 1/2))
dbinom(80:100, 100, prob = 1/2)
dbinom(80, 100, prob = 1/2)
dbinom(80:100, 100, prob = 1/2)
sum(dbinom(80:100, 100, prob = 1/2))
rbinom(1000, 100, .5)
hist(rbinom(1000, 100, .5))
sum(dbinom(80:100, 100, prob = 1/2))
sum(dbinom(60:100, 100, prob = 1/2))
## n = number of experiments
## size = number of trials per exp
##
hist(rbinom(n = 1000,
size = 100,
prob = 1/6))
rnorm(100)
?rnorm
# ?rnorm
## probability of being 1.25 SD above the mean?
## (for a standard normal distribution, mean = 0 / sd = 1)
pnorm(1.25, lower.tail = TRUE)
# ?rnorm
## probability of being 1.25 SD above the mean?
## (for a standard normal distribution, mean = 0 / sd = 1)
pnorm(1.25, lower.tail = FALSE)
1 - pnorm(1.25)
## probability of being 1.25 SD below the mean?
pnorm(1.25)
pnorm(132, 100, 15, lower.tail = FALSE)
pnorm(-1.25)
## probability of being 1.25 SD below the mean?
pnorm(1.25) # lower.tail= TRUE is the default
## probability of being 1.25 SD below the mean?
pnorm(1.25, lower.tail = TRUE) # lower.tail= TRUE is the default
## probability of being 1.25 SD below the mean?
pnorm(1.25, lower.tail = FALSE) # lower.tail= TRUE is the default
pnorm(-1.25)
qnorm(.10, 100, 15, lower.tail = FALSE)
qnorm(.10, 100, 15, lower.tail = TRUE)
?pt
?pf
?pchisq
?pgamma
mx <- matrix(c(4, 0, 0, 4), nrow = 2)
mx
mx <- matrix(c(4, 0, 0, 9), nrow = 2)
mx <- matrix(c(4, 0, 0, 9), nrow = 2)
MASS::mvrnorm(10, c(x = 8, y = 16), mx)
mx <- matrix(c(4, 0, 0, 9), nrow = 2)
MASS::mvrnorm(10, c(x = 8, y = 16), mx)
mx <- matrix(c(4, 1, 1, 9), nrow = 2)
MASS::mvrnorm(10, c(x = 8, y = 16), mx)
mx <- matrix(c(4, 0, 0, 9), nrow = 2)
MASS::mvrnorm(10, c(x = 8, y = 16), mx)
.2 * 2 * 3
mx <- matrix(c(4, 1.2, 1.2, 9), nrow = 2)
MASS::mvrnorm(10, c(x = 8, y = 16), mx)
.4 * 2 * 3
mx <- matrix(c(4, 2.4, 2.4, 9), nrow = 2)
MASS::mvrnorm(10, c(x = 8, y = 16), mx)
mx <- matrix(c(4, 2.4, 2.4, 9), nrow = 2)
MASS::mvrnorm(500, c(x = 8, y = 16), mx)
sim_4 <- MASS::mvrnorm(500, c(x = 8, y = 16), mx)
tibble(x = sim_4[, 1],
y = sim_4[, 2])
library("tibble")
tibble(x = sim_4[, 1],
y = sim_4[, 2])
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
if (!requireNamespace("webex")) {
stop("You must have the 'webex' package installed to knit HTML from this template.\n   devtools::install_github(\"psyteachr/webex\")")
} else {
library("webex")
}
# Chunk 2: import-data
library("tidyverse") # you're welcome
library("corrr")
stars <- read_csv("data/stars.csv",
col_types = cols(.default = col_integer()))
stars_long <- stars %>%
pivot_longer(I01:I51,
names_to = "item_id",
values_to = "score")
subscales <- read_csv("data/subscales.csv",
col_types = "cc")
grades <- read_csv("data/grades.csv", col_types = "ii")
setwd("/mnt/data/Tresors/priv/HU-stats-2021/public/05_regression/afternoon_multreg")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
if (!requireNamespace("webex")) {
stop("You must have the 'webex' package installed to knit HTML from this template.\n   devtools::install_github(\"psyteachr/webex\")")
} else {
library("webex")
}
# Chunk 2: import-data
library("tidyverse") # you're welcome
library("corrr")
stars <- read_csv("data/stars.csv",
col_types = cols(.default = col_integer()))
stars_long <- stars %>%
pivot_longer(I01:I51,
names_to = "item_id",
values_to = "score")
subscales <- read_csv("data/subscales.csv",
col_types = "cc")
grades <- read_csv("data/grades.csv", col_types = "ii")
stars_sub <- inner_join(stars_long, subscales, "item_id")
## calculate means for each subject
scale_means_by_sub <- stars_sub %>%
group_by(id, subscale) %>%
summarize(mean_score = mean(score, na.rm = TRUE),
.groups = "drop")
scale_means_by_sub
subscale_wide
## now put it back in wide format
subscale_wide <- scale_means_by_sub %>%
pivot_wider(names_from = subscale,
values_from = mean_score)
## calculate a correlation matrix
subscale_wide %>%
select(-id) %>%
correlate() %>%
shave() %>%
fashion()
subscale_wide
subscale_wide %>%
select(-id)
subscale_wide %>%
select(-id) %>%
pairs()
summary(mod_stan)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
if (!requireNamespace("webex")) {
stop("You must have the 'webex' package installed to knit HTML from this template.\n   devtools::install_github(\"psyteachr/webex\")")
} else {
library("webex")
}
# Chunk 2: import-data
library("tidyverse") # you're welcome
library("corrr")
stars <- read_csv("data/stars.csv",
col_types = cols(.default = col_integer()))
stars_long <- stars %>%
pivot_longer(I01:I51,
names_to = "item_id",
values_to = "score")
subscales <- read_csv("data/subscales.csv",
col_types = "cc")
grades <- read_csv("data/grades.csv", col_types = "ii")
# Chunk 3: subscale-means
stars_sub <- inner_join(stars_long, subscales, "item_id")
## calculate means for each subject
scale_means_by_sub <- stars_sub %>%
group_by(id, subscale) %>%
summarize(mean_score = mean(score, na.rm = TRUE),
.groups = "drop")
# Chunk 4: corr-mx
## now put it back in wide format
subscale_wide <- scale_means_by_sub %>%
pivot_wider(names_from = subscale,
values_from = mean_score)
## calculate a correlation matrix
subscale_wide %>%
select(-id) %>%
correlate() %>%
shave() %>%
fashion()
# Chunk 5: pairs
subscale_wide %>%
select(-id) %>%
pairs()
# Chunk 6: regression
dat <- subscale_wide %>%
inner_join(grades, "id")
## now run the regression
mod <- lm(grade ~ Ask_For_Help + Interpretation + Self_Concept +
Teacher + Test, dat)
summary(mod)
## standardize your predictors, figure out the "best" one
dat_stan <- dat %>%
mutate(ask_c = (Ask_For_Help - mean(Ask_For_Help)) / sd(Ask_For_Help),
interp_c = (Interpretation - mean(Interpretation)) / sd(Interpretation),
self_c = (Self_Concept - mean(Self_Concept)) / sd(Self_Concept),
teach_c = (Teacher - mean(Teacher)) / sd(Teacher),
test_c = (Test - mean(Test)) / sd(Test))
mod_stan <- lm(grade ~ ask_c + interp_c + self_c + teach_c + test_c , dat_stan)
summary(mod_stan)
coef(mod_stan)
which.max(abs(predictors))
predictors <- coef(mod_stan)
which.max(abs(predictors))
predictors <- coef(mod_stan)[-1] # -1 gets rid of the intercept
which.max(abs(predictors))
names(which.max(abs(predictors)))
which.max(abs(predictors)) %>% names()
grades
inner_join(grades, software, "id")
software <- read_csv("data/software.csv")
software <- read_csv("data/software.csv", col_types = "ic")
software
grade_soft <- inner_join(grades, software, "id")
mod_gsoft <- lm(grade ~ software, grade_soft)
grade_soft <- inner_join(grades, software, "id")
grades
software
grade_soft <- inner_join(grades, software, c("id" = "ID"))
mod_gsoft <- lm(grade ~ software, grade_soft)
summary(mod_gsoft)
coef(mod_gsoft)[[2]]
coef(mod_gsoft)[[2]] %>% round(2)
grades
read_delim("data/stars_survey.txt", "\t") %>%
separate(item_id, c("pre", "num"), 1, convert = TRUE) %>%
mutate(item_id = sprintf("I%02d", num)) %>%
select(item_id, item) %>% knitr::kable()
read_delim("data/stars_survey.txt", "\t") %>%
separate(item_id, c("pre", "num"), 1, convert = TRUE) %>%
mutate(item_id = sprintf("I%02d", num)) %>%
select(item_id, item, question_type = scale) %>% knitr::kable()
x <- rnorm(50, -5, 1)
y <- rnorm(50, -3, 1)
t.test(x, y)
x <- rnorm(50, -5, 1)
y <- rnorm(50, -5.4, 1)
t.test(x, y)
